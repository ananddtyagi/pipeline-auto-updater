from llama_index.core.prompts import (
    PromptTemplate
)
from llama_index.core.bridge.pydantic import BaseModel, Field

# --- CORRECTNESS ---
CORRECTNESS_EVALUATION_TEMPLATE = '''
    You are an expert evaluation system for a question-answering chatbot. Your task is to judge the correctness of generated answers by comparing them to reference answers, considering the original user query.

    You will be provided with the following information:
    <user_query>
    [User's original question]
    </user_query>

    <reference_answer>
    [The correct answer to be used as the ultimate source of truth]
    </reference_answer>

    <generated_answer>
    [The answer generated by the chatbot that needs evaluation]
    </generated_answer>

    Your goal is to determine if the generated answer is correct, or incorrect based on the reference answer and the context of the user query. Follow these steps to complete your evaluation:

    1. Carefully read and analyze the user query, reference answer, and generated answer.
    2. Break down your evaluation process inside <analysis> tags:
    a. Identify and list the key points and important information in the reference answer.
    b. Compare each key point from the reference answer to the generated answer.
    c. Note any additional relevant information provided in the generated answer.
    d. Assess the overall accuracy and completeness of the generated answer.

    3. Determine if the generated answer either passes or fails:
    - PASS: The important parts of the response are consistent with the reference answer and adequately address the user query.
    - FAIL: The answer is partially or fundamentally wrong, it may be missing key points or contains major inaccuracies, or fails to address the user query.

    4. Provide your final evaluation in the following format:
    <evaluation>
    Result: [PASS/FAIL]
    Feedback: [Detailed explanation of your evaluation, highlighting strengths and weaknesses of the generated answer]
    </evaluation>

    Remember to focus on the important parts of the response being consistent with the reference answer, rather than requiring exact matches. Consider partial correctness when evaluating the answer, and provide specific feedback on any discrepancies or inaccuracies.

    Here are some examples to help you understand the evaluation process:
    <examples>
    <example>
    <user_query>
    What is the capital of France?
    </user_query>

    <reference_answer>
    The capital of France is Paris.
    </reference_answer>
    <generated_answer>
    The capital of France is Paris.
    </generated_answer>

    <ideal_output>
    <analysis>
    a. Key points in the reference answer:
    - The capital of France is Paris.

    b. Comparison of key points:
    - The generated answer correctly states that the capital of France is Paris.

    c. Additional relevant information in the generated answer:
    - No additional information is provided, but none is needed.

    d. Overall accuracy and completeness assessment:
    - The generated answer is accurate and complete.

    After careful analysis, I can conclude that the generated answer is correct.

    </analysis>
    <evaluation>
    Result: PASS
    Feedback: The generated answer accurately states that the capital of France is Paris, which is consistent with the reference answer. The response is both accurate and complete.
    </evaluation>
    </ideal_output>
    </example>

    <example>
    <user_query>
    What is the largest planet in our solar system?
    </user_query>

    <reference_answer>
    The largest planet in our solar system is Jupiter.
    </reference_answer>
    <generated_answer>
    The largest planet in our solar system is Saturn.
    </generated_answer>

    <ideal_output>
    <analysis>
    a. Key points in the reference answer:
    - The largest planet in our solar system is Jupiter.

    b. Comparison of key points:
    - The generated answer incorrectly states that the largest planet is Saturn, while the reference answer correctly states it is Jupiter.

    c. Additional relevant information in the generated answer:
    - No additional relevant information is provided.

    d. Overall accuracy and completeness assessment:
    - The generated answer is incorrect as it provides the wrong information.

    After careful analysis, I can conclude that the generated answer is incorrect.

    </analysis>
    <evaluation>
    Result: FAIL
    Feedback: The generated answer incorrectly states that the largest planet in our solar system is Saturn, while the correct answer is Jupiter. This is a significant error, and the response fails to meet the correctness criteria.
    </evaluation>
    </ideal_output>
    </example>
    </examples>


    Now, please proceed with your analysis and evaluation of the provided query and answers.
    <user_query>
    {query}
    </user_query>

    <reference_answer>
    {reference_answer}
    </reference_answer>

    <generated_answer>
    {generated_answer}
    </generated_answer>

    <analysis>
    '''

# --- FAITHFULNESS ---

FAITHFULNESS_EVALUATION_TEMPLATE = PromptTemplate(
    '''Please tell if a given piece of information is supported by the context.
    You need to answer with either YES or NO.
    Answer YES if **any part** of the context supports the information, even if most of the context is unrelated.
    Answer NO if the context does not support the information at all.
    Be sure to read all provided context segments carefully before making your decision.

    Some examples are provided below:

    Example 1:
    Information: The Eiffel Tower is located in Paris.
    Context: The Eiffel Tower, a symbol of French culture, stands prominently in the city of Paris.
    Answer: YES

    Example 2:
    Information: Bananas are a type of berry.
    Context: Bananas are a popular fruit enjoyed worldwide and are rich in potassium.
    Answer: NO

    Example 3:
    Information: Cats are reptiles.
    Context: Cats are domesticated felines known for their agility and companionship.
    Answer: NO

    Example 4:
    Information: Amazon started as an online bookstore.
    Context: Amazon initially launched as an online store for books but has since expanded into a global e-commerce giant
    offering various products and services.
    Answer: YES

    Information: {query_str}
    Context: {context_str}
    Answer:'''
    )

FAITHFULNESS_REFINE_TEMPLATE = PromptTemplate(
    'We want to understand if the following information is present ' + 
    'in the context information: {query_str}\n' +
    'We have provided an existing YES/NO answer: {existing_answer}\n' +
    'We have the opportunity to refine the existing answer ' +
    '(only if needed) with some more context below.\n' +
    '------------\n' +
    '{context_msg}\n' +
    '------------\n' +
    'If the existing answer was already YES, still answer YES. ' +
    'If the information is present in the new context, answer YES. ' +
    'Otherwise answer NO.\n'
)


# --- RELEVANCY ---

RELEVANCY_EVALUATION_TEMPLATE = '''
    Your task is to evaluate if the response for the query \
    is in line with the context information provided.\n
    You have two options to answer. Either YES/ NO.\n
    Answer - YES, if the response for the query \
    is in line with context information otherwise NO.\n
    Query and Response: \n {query_str}\n
    Context: \n {context_str}\n
    Answer:  
    '''

RELEVANCY_REFINE_TEMPLATE = '''
    We want to understand if the following query and response is
    in line with the context information: \n {query_str}\n
    We have provided an existing YES/NO answer: \n {existing_answer}\n
    We have the opportunity to refine the existing answer 
    (only if needed) with some more context below.\n
    ------------\n
    {context_msg}\n
    ------------\n
    If the existing answer was already YES, still answer YES. 
    If the information is present in the new context, answer YES. 
    Otherwise answer NO.\n 
    '''

# --- GUIDELINE ---

class EvaluationData(BaseModel):
    passing: bool = Field(description='Whether the response passes the guidelines.')
    score: int  = Field(description='A score of either 1,2,3,4 or 5 that determines how well the guidelines were followed')
    feedback: str = Field(
        description='The feedback for the response based on the guidelines.'
    )

GENERAL_GUIDELINES = [(
    "The response should be professional while also maintaining a somewhat friendly tone.\n"
    "The response should stay on topic and either answer the question if it has the necessary information to do so.\n"
    "If the response mentions that it does not have the necessary information to answer, it should say so and respond appropriately.\n"
    "The response should provide specific numerical details if the user asks OR say that it cannot do so for a specific reason.\n"
)]


GUIDELINES_EVALUATION_TEMPLATE = '''
    You are an expert evaluation system for a question-answering chatbot. Your task is to judge if the generated answer follows the guidelines given.

    You will be provided with the following information:
    <user_query>
    [User's original question]
    </user_query>

    <generated_answer>
    [The answer generated by the chatbot that needs evaluation]
    </generated_answer>


    <guidelines>
    [The guidelines that the response needs to adhere to]
    </guidelines>

    Your goal is to determine if the generated answer does adhere to the guidelines that are provided.

    1. Carefully read and analyze the user query, generated answer, and guidelines.
    2. Break down your evaluation process inside <analysis> tags:
        a. Identify and list the key points and important information in the guidelines.
        b. Compare each key point from the guidelines to the generated answer.
        c. Note any additional relevant information provided in the generated answer.
        d. Assess the overall accuracy and completeness of the generated answer.

    3. Determine if the generated answer is:
    - PASS: The generated answer adheres to the guidelines.
    - Incorrect: The generated answer does not adhere to the guidelines.

    4. Provide your final evaluation in the following format:
    <evaluation>
    Result: [PASS/FAIL]
    Feedback: [Detailed explanation of your evaluation, highlighting strengths and weaknesses of the generated answer]
    </evaluation>

     Now, please proceed with your analysis and evaluation of the provided query and answers.
    <user_query>
    {query}
    </user_query>
    
    <generated_answer>
    {generated_answer}
    </generated_answer>
    
    <guidelines>
    {guidelines}
    </guidelines>

    <analysis>
'''

GUIDELINES_CHOOSING_TEMPLATE = '''
    You are an expert in decision making. 
    Your task is to determine if these guidelines are relevant and pertain to the given query and generated answer.
    You will be provided with the following information:
    <user_query>
    [User's original question]
    </user_query>

    <generated_answer>
    [The answer generated by the chatbot that needs evaluation]
    </generated_answer>

    <guidelines>
    [The guidelines that the response needs to adhere to]
    </guidelines>
    
    <evaluation>
    Relevant: [YES/NO]
    </evaluation>
    
    Your goal is to determine if the query and generated_answer are relevant to the guidelines.

    1. Carefully read and analyze the user query, generated answer, and guidelines.
    2. Break down your evaluation process inside <analysis> tags:
        a. Identify the key requirements the guideline is asking for.
        b. Determine if the query asked would require the answer to follow the guideline.
        c. Determine if the generated answer would need to follow the guideline. 
        
    2. Determine if the generated answer is relevant to the guidelines:
    - YES: The query and generated_answer are relevant to the guidelines and need to follow the guideline.
    - NO: The query and generated_answer are not relevant to the guidelines and would not need to follow the guideline.

    3. Provide your final evaluation in the following format:
    <analysis>
    [The thought process determining if the query and generated answer need to follow the given guideline]
    </analysis>
    
    <evaluation>   
    Relevant: [YES/NO]
    </evaluation>

    Now, please proceed with your analysis and evaluation of the provided query and answers.
    <user_query>
    {query}
    </user_query>
    
    <generated_answer>
    {generated_answer}
    </generated_answer>
    
    <guidelines>
    {guidelines}
    </guidelines>
    
    <analysis>

    '''